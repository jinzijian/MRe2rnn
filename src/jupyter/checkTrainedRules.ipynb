{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import sys\n",
    "from copy import deepcopy, copy\n",
    "import pickle\n",
    "sys.path.append('../../src')\n",
    "from data import read_ATIS, load_pkl, load_glove_embed\n",
    "from tensorly import cp_tensor, decomposition, tenalg\n",
    "from rules.fsa_to_tensor import Automata\n",
    "from rules.create_logic_mat_bias import create_mat_and_bias_with_empty_ATIS\n",
    "from data import load_dict_ATIS, load_classification_dataset\n",
    "from rules.tensor_func import tensor3_to_factors\n",
    "from rules.fsa_to_tensor import dfa_to_tensor\n",
    "from RE import PredictByRE1\n",
    "from utils.utils import Args\n",
    "import os\n",
    "pd.set_option('display.max_rows', 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../../data/TREC/automata/automata.split.randomseed.200.False.0.0021.0.pkl\n",
      "(8986, 200)\n",
      "0.3\n"
     ]
    }
   ],
   "source": [
    "# ATIS\n",
    "config_path = '../../model/11-6/1106090555-1604653555.7235456.res' \n",
    "model_path = '../../model/11-6/D0.9739-T0.9552-DI0.8655-TI0.8712-1106090555-1604653555.7040935-ATIS-0.model'\n",
    "dfa_path = '../../data/ATIS/automata/automata.newrule.split.randomseed150.False.0.0003.0.pkl'\n",
    "dataset = 'ATIS'\n",
    "\n",
    "# # QC\n",
    "config_path = '../../model/11-6/1106092247-1604654567.165155.res' \n",
    "model_path = '../../model/11-6/D0.8820-T0.8940-DI0.5020-TI0.5920-1106092247-1604654567.126257-TREC-0.model'\n",
    "dfa_path = '../../data/TREC/automata/automata.split.randomseed.200.False.0.0021.0.pkl'\n",
    "dataset = 'TREC'\n",
    "\n",
    "\n",
    "\n",
    "_all = pickle.load(open(config_path,'rb'))\n",
    "results_dict = _all['res']\n",
    "args_dict = _all['args']\n",
    "config = args_dict\n",
    "\n",
    "model_parameters = torch.load(model_path, map_location='cpu')\n",
    "\n",
    "\n",
    "automata_dicts = load_pkl(dfa_path)\n",
    "dset = load_classification_dataset(dataset, '../../data/')\n",
    "word2idx = dset['t2i']\n",
    "automata = automata_dicts['automata']\n",
    "V_embed, D1, D2 = automata_dicts['V'], automata_dicts['D1'], automata_dicts['D2']\n",
    "language_tensor1, state2idx, wildcard_mat1, language1 = dfa_to_tensor(automata, word2idx)\n",
    "complete_tensor = language_tensor1 + wildcard_mat1\n",
    "print(V_embed.shape)\n",
    "print(config['beta'])\n",
    "\n",
    "D1_new = model_parameters['fsa_rnn.trans_r_1'].numpy()\n",
    "D2_new = model_parameters['fsa_rnn.trans_r_2'].numpy()\n",
    "V_embed_new = model_parameters['fsa_rnn.embed_r.weight'].numpy()[:-1] # V+1 R => V, R\n",
    "word_embed_new = model_parameters['fsa_rnn.embedding.weight'].numpy()[:-1]# V+1 D => V, D\n",
    "V_embed_generalized_new = model_parameters['fsa_rnn.embed_r_generalized'].numpy() # D x R\n",
    "wildcard_mat_new = model_parameters['fsa_rnn.trans_wildcard'].numpy()\n",
    "bias_new = model_parameters['bias'].numpy()\n",
    "mat_new = model_parameters['mat'].numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.04113941,  0.00578136,  0.0214278 , ..., -0.08589418,\n",
       "        -0.11406953, -0.01329235],\n",
       "       [ 0.01657075, -0.0152354 ,  0.02324565, ..., -0.01573091,\n",
       "        -0.01982415, -0.03651084],\n",
       "       [-0.01272624, -0.02381838, -0.01476424, ..., -0.00373371,\n",
       "         0.00577647, -0.04704226],\n",
       "       ...,\n",
       "       [-0.04664796,  0.05202688,  0.02019971, ..., -0.0008244 ,\n",
       "         0.0081084 , -0.02956333],\n",
       "       [-0.04258694,  0.04758633,  0.02350235, ..., -0.0096619 ,\n",
       "         0.00265078, -0.02633813],\n",
       "       [-0.00798957,  0.01635643, -0.01469606, ..., -0.04155448,\n",
       "        -0.03224054,  0.02322396]], dtype=float32)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "D2_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "V_embed_generalized_recovered = np.dot(word_embed_new, V_embed_generalized_new)\n",
    "V_embed_new_recovered = V_embed_generalized_recovered * (1 - config['beta']) + config['beta'] * V_embed_new\n",
    "recovered_language_tensor = cp_tensor.cp_to_tensor([None,[V_embed_new_recovered, D1_new, D2_new]]) \n",
    "complete_recovered_tensor = recovered_language_tensor + wildcard_mat_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "685485.0\n",
      "665722.0\n"
     ]
    }
   ],
   "source": [
    "new_rule_tensor = np.zeros_like(complete_recovered_tensor)\n",
    "gamma = 0.3  # 0.7 for default ATIS\n",
    "new_rule_tensor[complete_recovered_tensor > gamma] = 1\n",
    "new_rule_tensor[complete_recovered_tensor < gamma] = 0\n",
    "# new_rule_tensor = complete_recovered_tensor\n",
    "print(new_rule_tensor.sum())\n",
    "print(complete_tensor.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    'complete_tensor': new_rule_tensor,\n",
    "    'mat': mat_new,\n",
    "    'bias': bias_new,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "config['bz'] = 500\n",
    "args = Args(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INIT DEV ACC: 0.502\n",
      "INIT TEST ACC: 0.592\n"
     ]
    }
   ],
   "source": [
    "print('INIT DEV ACC: {}'.format(results_dict[int(args.seed)][0]))\n",
    "print('INIT TEST ACC: {}'.format(results_dict[int(args.seed)][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained RE ACC\n",
      "max_len: 39, avg_len: 12.213695871097684\n",
      "max_len: 33, avg_len: 12.1\n",
      "max_len: 19, avg_len: 9.534\n",
      "(8987, 94, 94)\n",
      "RE TRAIN ACC\n",
      "total acc: 0.6201409869083585\n",
      "0.6201409869083585\n",
      "RE DEV ACC\n",
      "total acc: 0.592\n",
      "0.592\n",
      "RE TEST ACC\n",
      "total acc: 0.71\n",
      "0.7100000000000001\n"
     ]
    }
   ],
   "source": [
    "dset = load_classification_dataset(dataset, '../../data/')\n",
    "print('Trained RE ACC')\n",
    "_ , _,_,_,_,_ = PredictByRE1(args, params,  dset,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args.seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8986\n"
     ]
    }
   ],
   "source": [
    "print(dset['t2i']['<pad>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reset transitions based on the trained automata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "t2i = dset['t2i']\n",
    "i2t = dset['i2t']\n",
    "automata_new = Automata()\n",
    "automata_origin = Automata()\n",
    "for k, v in automata.items():\n",
    "    setattr(automata_new, k, v)\n",
    "    setattr(automata_origin, k, v)\n",
    "automata_new.transitions = dict()\n",
    "n_vocab, n_state, _ = new_rule_tensor.shape\n",
    "for s_from in range(n_state):\n",
    "    for s_to in range(n_state):\n",
    "        trans_info = new_rule_tensor[:,s_from, s_to]\n",
    "        N = sum(trans_info) \n",
    "        if N >= len(t2i)-1:\n",
    "            inp = '$'.format(N)\n",
    "            automata_new.addtransition(s_from, s_to, inp)\n",
    "        elif N > 15:\n",
    "            inp = '${}'.format(N)\n",
    "            automata_new.addtransition(s_from, s_to, inp)\n",
    "        elif ((N > 0) and (N <= 15)):\n",
    "            inp = set([i2t[i] for i in range(len(trans_info)) if trans_info[i]])\n",
    "            automata_new.addtransition(s_from, s_to, inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "automata_new.drawGraph(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.python.org/doc/essays/graphs/\n",
    "def find_all_paths(transition_graph, start, end, path=[]):\n",
    "    path = path + [start]\n",
    "    if start == end:\n",
    "        return [path]\n",
    "\n",
    "    paths = []\n",
    "    tos = transition_graph[start]\n",
    "    \n",
    "    nodes = [i for i in range(len(tos)) if tos[i] ]\n",
    "    \n",
    "    for node in nodes:\n",
    "        \n",
    "        if node not in path:\n",
    "            newpaths = find_all_paths(transition_graph, node, end, path)\n",
    "            for newpath in newpaths:\n",
    "                paths.append(newpath)\n",
    "    return paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'ENTITY': [5, 7, 13],\n",
       " 'DESCRIPTION': [17, 21, 23, 26, 30, 36, 37, 38, 39],\n",
       " 'LOCATION': [42, 45, 50, 43],\n",
       " 'HUMAN': [53, 56, 57, 65],\n",
       " 'NUMERIC': [71],\n",
       " 'ABBREVIATION': [88]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "automata['finalstates_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_partial_automata(based_automata, label, new_rule_tensor):\n",
    "\n",
    "    automata_partial = Automata()\n",
    "    automata_partial.setstartstate(0)\n",
    "    tensor_sum = new_rule_tensor.sum(0)\n",
    "    transition_graph = np.zeros_like(tensor_sum)\n",
    "    transition_graph[tensor_sum > 0.99] = 1\n",
    "\n",
    "    all_paths = []\n",
    "    goals = based_automata.finalstates_label[label]\n",
    "    for goal in goals[0:3] :\n",
    "        paths = find_all_paths(transition_graph, 0, goal)\n",
    "        all_paths += paths\n",
    "        automata_partial.addfinalstates(goal)\n",
    "        for path in paths:\n",
    "            for i in range(len(path)-1):\n",
    "                try:\n",
    "                    inps = based_automata.transitions[path[i]][path[i+1]]\n",
    "                except:\n",
    "                    continue\n",
    "                automata_partial.addtransition(path[i], path[i+1], inps)        \n",
    "                if path[i] in based_automata.transitions[path[i]]:\n",
    "                    inps = based_automata.transitions[path[i]][path[i]]\n",
    "                    automata_partial.addtransition(path[i], path[i], inps)\n",
    "                if path[i+1] in based_automata.transitions[path[i+1]]:\n",
    "                    inps = based_automata.transitions[path[i+1]][path[i+1]]\n",
    "                    automata_partial.addtransition(path[i+1], path[i+1], inps)\n",
    "\n",
    "    print(all_paths)\n",
    "    return automata_partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'capacity'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c3b3ff60a818>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'capacity'\u001b[0m \u001b[0;31m# for ATIS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mautomata_partial\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_partial_automata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mautomata_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_rule_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-17-855863e470be>\u001b[0m in \u001b[0;36mget_partial_automata\u001b[0;34m(based_automata, label, new_rule_tensor)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mall_paths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mgoals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbased_automata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinalstates_label\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mgoal\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgoals\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_all_paths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtransition_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgoal\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'capacity'"
     ]
    }
   ],
   "source": [
    "label = 'capacity' # for ATIS\n",
    "automata_partial = get_partial_automata(automata_new, label, new_rule_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "automata_partial.drawGraph(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch1.3",
   "language": "python",
   "name": "pytorch1.3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
